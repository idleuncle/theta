{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUE-CLUENER 细粒度命名实体识别\n",
    "\n",
    "本数据是在清华大学开源的文本分类数据集THUCTC基础上，选出部分数据进行细粒度命名实体标注，原数据来源于Sina News RSS.\n",
    "\n",
    "训练集：10748 验证集：1343\n",
    "\n",
    "标签类别：\n",
    "数据分为10个标签类别，分别为: 地址（address），书名（book），公司（company），游戏（game），政府（goverment），电影（movie），姓名（name），组织机构（organization），职位（position），景点（scene）\n",
    "\n",
    "数据下载地址：https://github.com/CLUEbenchmark/CLUENER2020\n",
    "\n",
    "排行榜地址：https://cluebenchmarks.com/ner.html\n",
    "\n",
    "span 79.530 (seed=42,no_kd,no_augs) 80.207 (seed=8864, robert-wwm-large-ext)\n",
    "42/2augs: 77.771\n",
    "\n",
    "enable_kd: 79.254\n",
    "\n",
    "roberta-wwm-large-ext-chinese: 80.293 focalloss1.5: 79.824 / 2augs|CrossEntropyLoss: 79.590\n",
    "\n",
    "pn: 78.751\n",
    "\n",
    "2020-11-16 13:39:54.678 | INFO     | theta.modeling.ner_utils:ner_evaluate:1083 - total_right: 2499, total_preds: 2908, total_targets: 2982\n",
    "\n",
    "2020-11-16 13:39:54.680 | WARNING  | theta.modeling.ner_utils:ner_evaluate:1090 - Micro: P: 0.853501, R: 0.842357, F1: 0.835674\n",
    "\n",
    "2020-11-16 13:39:54.681 | WARNING  | theta.modeling.ner_utils:ner_evaluate:1096 - Macro: P: 0.859354, R: 0.838028, F1: 0.848557\n",
    "\n",
    "\n",
    "|模型|线上效果f1|\n",
    "|------|------:|\n",
    "|Bert-base|78.82|\n",
    "|RoBERTa-wwm-large-ext|80.42|\n",
    "|Bi-Lstm + CRF|70.00|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型IO定义\n",
    "\n",
    "通常将本节代码写入cluener.py文件中，这也是开发者主要要编写的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labels = [\n",
    "    'address', 'book', 'company', 'game', 'government', 'movie', 'name',\n",
    "    'organization', 'position', 'scene'\n",
    "]\n",
    "ner_connections = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if text:\n",
    "        text = text.strip()\n",
    "    return text\n",
    "\n",
    "def cluener_data_generator(train_file, desc=\"\"):\n",
    "    for i, line in enumerate(tqdm(open(train_file).readlines(), desc=desc)):\n",
    "        guid = f\"{i}\"\n",
    "        json_data = json.loads(line.strip())\n",
    "        text = clean_text(json_data['text'])\n",
    "\n",
    "        tags = []\n",
    "        classes = json_data['label'].keys()\n",
    "        for c in classes:\n",
    "            c_labels = json_data['label'][c]\n",
    "            for label, span in c_labels.items():\n",
    "                x0, x1 = span[0]\n",
    "                s = x0\n",
    "                m = text[x0:x1 + 1]\n",
    "                tags.append({'category': c, 'start': s, 'mention': m})\n",
    "        yield guid, text, None, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练集/验证集数据生成器\n",
    "要点是每条样本返回(yield)(guid, text, None, tags)元组。\n",
    "验证集数据生成器不是必须的，当未提供时，Theta自动从train_data_generator生成的数据集中切分训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(train_file):\n",
    "    if train_file is None:\n",
    "        train_file = 'data/train.json'\n",
    "\n",
    "    for guid, text, _, tags in cluener_data_generator(train_file, desc=\"Train data\"):\n",
    "        yield guid, text, None, tags\n",
    "\n",
    "    eval_file = 'data/dev.json'\n",
    "    for guid, text, _, tags in cluener_data_generator(eval_file, desc=\"Eval data\"):\n",
    "        yield guid, text, None, tags\n",
    "        \n",
    "def eval_data_generator(eval_file):\n",
    "    if eval_file is None:\n",
    "        eval_file = 'data/dev.json'\n",
    "\n",
    "    for guid, text, _, tags in cluener_data_generator(eval_file, desc=\"Eval data\"):\n",
    "        yield guid, text, None, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义测试集数据生成器\n",
    "要点是每条样本返回(guid, text, None, None)元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_generator(test_file):\n",
    "    if test_file is None:\n",
    "        test_file = 'data/test.json'\n",
    "\n",
    "    for i, line in enumerate(\n",
    "            tqdm(open(test_file).readlines(), desc=\"Test data: \")):\n",
    "        guid = f\"{i}\"\n",
    "        json_data = json.loads(line.strip())\n",
    "        text = clean_text(json_data['text'])\n",
    "\n",
    "        yield guid, text, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义提交结果文件生成函数\n",
    "完成训练、推理后生成reviews_file标准格式输出文件，在此处转换成需要的输出文件格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(args):\n",
    "    reviews_file = args.reviews_file\n",
    "    reviews = json.load(open(reviews_file, 'r'))\n",
    "\n",
    "    submission_file = f\"./submissions/{args.dataset_name}_predict.json\"\n",
    "    test_results = []\n",
    "    for guid, json_data in tqdm(reviews.items()):\n",
    "        text = json_data['text']\n",
    "\n",
    "        classes = {}\n",
    "        for json_entity in json_data['tags']:\n",
    "            c = json_entity['category']\n",
    "            s = json_entity['start']\n",
    "            m = json_entity['mention']\n",
    "            if c not in classes:\n",
    "                classes[c] = {}\n",
    "            if m not in classes[c]:\n",
    "                classes[c][m] = []\n",
    "            classes[c][m].append([s, s + len(m) - 1])\n",
    "        test_results.append({'id': guid, 'text': text, 'label': classes})\n",
    "\n",
    "    with open(submission_file, 'w') as wt:\n",
    "        for line in test_results:\n",
    "            wt.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")\n",
    "\n",
    "    logger.info(f\"Saved {len(reviews)} lines in {submission_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义主应用程序\n",
    "此处通常无需修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- NerApp --------------------\n",
    "from theta.modeling.app import NerApp\n",
    "\n",
    "\n",
    "class MyApp(NerApp):\n",
    "    def __init__(self,\n",
    "                 experiment_params,\n",
    "                 ner_labels: list,\n",
    "                 ner_connections: list,\n",
    "                 add_special_args=None):\n",
    "\n",
    "        super(MyApp, self).__init__(experiment_params, ner_labels,\n",
    "                                    ner_connections, add_special_args)\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        train_data_generator,\n",
    "        test_data_generator,\n",
    "        generate_submission=None,\n",
    "        eval_data_generator=None,\n",
    "    ):\n",
    "\n",
    "        args = self.args\n",
    "\n",
    "        if args.preapre_data:\n",
    "            logger.info(f\"Prepare data.\")\n",
    "        else:\n",
    "            super(MyApp, self).run(train_data_generator, test_data_generator,\n",
    "                                   generate_submission, eval_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主入口函数\n",
    "提供缺省的主入口函数，通常无需修改即可正常运行。\n",
    "\n",
    "其中add_special_args()函数可以加入自己需要的命令行参数定义，实现自定义的控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Main --------------------\n",
    "def main():\n",
    "    # -------- Customized arguments --------\n",
    "    def add_special_args(parser):\n",
    "        parser.add_argument(\"--preapre_data\",\n",
    "                            action='store_true',\n",
    "                            help=\"Preapre data.\")\n",
    "        return parser\n",
    "\n",
    "    app = MyApp(experiment_params,\n",
    "                ner_labels=ner_labels,\n",
    "                ner_connections=ner_connections,\n",
    "                add_special_args=add_special_args)\n",
    "\n",
    "    app.run(train_data_generator,\n",
    "            test_data_generator,\n",
    "            generate_submission=generate_submission,\n",
    "            eval_data_generator=None)\n",
    "            #eval_data_generator=eval_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-17 03:08:31.845 | DEBUG    | theta.modeling.utils:debug:26 - adam_epsilon: 1e-08\n",
      "2020-12-17 03:08:31.848 | DEBUG    | theta.modeling.utils:debug:26 - allow_overlap: False\n",
      "2020-12-17 03:08:31.849 | DEBUG    | theta.modeling.utils:debug:26 - artifact_path: None\n",
      "2020-12-17 03:08:31.850 | DEBUG    | theta.modeling.utils:debug:26 - aug_train_only: False\n",
      "2020-12-17 03:08:31.851 | DEBUG    | theta.modeling.utils:debug:26 - best_index: f1\n",
      "2020-12-17 03:08:31.852 | DEBUG    | theta.modeling.utils:debug:26 - brat_data_dir: None\n",
      "2020-12-17 03:08:31.853 | DEBUG    | theta.modeling.utils:debug:26 - cc: None\n",
      "2020-12-17 03:08:31.854 | DEBUG    | theta.modeling.utils:debug:26 - confidence: 0.0\n",
      "2020-12-17 03:08:31.855 | DEBUG    | theta.modeling.utils:debug:26 - dataset_name: cluener\n",
      "2020-12-17 03:08:31.856 | DEBUG    | theta.modeling.utils:debug:26 - diceloss_weight: None\n",
      "2020-12-17 03:08:31.857 | DEBUG    | theta.modeling.utils:debug:26 - emotion_words_file: None\n",
      "2020-12-17 03:08:31.858 | DEBUG    | theta.modeling.utils:debug:26 - enable_kd: False\n",
      "2020-12-17 03:08:31.859 | DEBUG    | theta.modeling.utils:debug:26 - enable_nested_entities: False\n",
      "2020-12-17 03:08:31.860 | DEBUG    | theta.modeling.utils:debug:26 - enable_sda: False\n",
      "2020-12-17 03:08:31.861 | DEBUG    | theta.modeling.utils:debug:26 - eval_file: None\n",
      "2020-12-17 03:08:31.862 | DEBUG    | theta.modeling.utils:debug:26 - eval_max_seq_length: 64\n",
      "2020-12-17 03:08:31.863 | DEBUG    | theta.modeling.utils:debug:26 - experiment_name: CLUE\n",
      "2020-12-17 03:08:31.864 | DEBUG    | theta.modeling.utils:debug:26 - focalloss_alpha: None\n",
      "2020-12-17 03:08:31.865 | DEBUG    | theta.modeling.utils:debug:26 - focalloss_gamma: 1.5\n",
      "2020-12-17 03:08:31.866 | DEBUG    | theta.modeling.utils:debug:26 - fold: 0\n",
      "2020-12-17 03:08:31.867 | DEBUG    | theta.modeling.utils:debug:26 - fp16: True\n",
      "2020-12-17 03:08:31.868 | DEBUG    | theta.modeling.utils:debug:26 - is_english: False\n",
      "2020-12-17 03:08:31.869 | DEBUG    | theta.modeling.utils:debug:26 - kd_coeff: 1.0\n",
      "2020-12-17 03:08:31.870 | DEBUG    | theta.modeling.utils:debug:26 - kd_decay: 0.995\n",
      "2020-12-17 03:08:31.871 | DEBUG    | theta.modeling.utils:debug:26 - learning_rate: 2e-05\n",
      "2020-12-17 03:08:31.872 | DEBUG    | theta.modeling.utils:debug:26 - loss_type: CrossEntropyLoss\n",
      "2020-12-17 03:08:31.872 | DEBUG    | theta.modeling.utils:debug:26 - max_train_examples: 0\n",
      "2020-12-17 03:08:31.873 | DEBUG    | theta.modeling.utils:debug:26 - model_path: /opt/share/pretrained/pytorch/bert-base-chinese\n",
      "2020-12-17 03:08:31.874 | DEBUG    | theta.modeling.utils:debug:26 - model_type: bert\n",
      "2020-12-17 03:08:31.875 | DEBUG    | theta.modeling.utils:debug:26 - num_augments: 2\n",
      "2020-12-17 03:08:31.876 | DEBUG    | theta.modeling.utils:debug:26 - num_train_epochs: 6\n",
      "2020-12-17 03:08:31.878 | DEBUG    | theta.modeling.utils:debug:26 - per_gpu_eval_batch_size: 32\n",
      "2020-12-17 03:08:31.880 | DEBUG    | theta.modeling.utils:debug:26 - per_gpu_predict_batch_size: 32\n",
      "2020-12-17 03:08:31.880 | DEBUG    | theta.modeling.utils:debug:26 - per_gpu_train_batch_size: 32\n",
      "2020-12-17 03:08:31.881 | DEBUG    | theta.modeling.utils:debug:26 - predict_max_seq_length: 256\n",
      "2020-12-17 03:08:31.882 | DEBUG    | theta.modeling.utils:debug:26 - random_type: None\n",
      "2020-12-17 03:08:31.883 | DEBUG    | theta.modeling.utils:debug:26 - sda_coeff: 1.0\n",
      "2020-12-17 03:08:31.884 | DEBUG    | theta.modeling.utils:debug:26 - sda_decay: 0.995\n",
      "2020-12-17 03:08:31.885 | DEBUG    | theta.modeling.utils:debug:26 - sda_empty_first: False\n",
      "2020-12-17 03:08:31.886 | DEBUG    | theta.modeling.utils:debug:26 - sda_stategy: recent_models\n",
      "2020-12-17 03:08:31.887 | DEBUG    | theta.modeling.utils:debug:26 - sda_teachers: 3\n",
      "2020-12-17 03:08:31.888 | DEBUG    | theta.modeling.utils:debug:26 - seed: 42\n",
      "2020-12-17 03:08:31.889 | DEBUG    | theta.modeling.utils:debug:26 - seg_backoff: 0\n",
      "2020-12-17 03:08:31.889 | DEBUG    | theta.modeling.utils:debug:26 - seg_len: 62\n",
      "2020-12-17 03:08:31.891 | DEBUG    | theta.modeling.utils:debug:26 - test_file: None\n",
      "2020-12-17 03:08:31.892 | DEBUG    | theta.modeling.utils:debug:26 - tracking_uri: None\n",
      "2020-12-17 03:08:31.893 | DEBUG    | theta.modeling.utils:debug:26 - train_file: None\n",
      "2020-12-17 03:08:31.894 | DEBUG    | theta.modeling.utils:debug:26 - train_max_seq_length: 64\n",
      "2020-12-17 03:08:31.895 | DEBUG    | theta.modeling.utils:debug:26 - train_rate: 0.9\n",
      "2020-12-17 03:08:31.895 | DEBUG    | theta.modeling.utils:debug:26 - train_sample_rate: 1.0\n",
      "2020-12-17 03:08:31.896 | DEBUG    | theta.modeling.utils:debug:26 - weight_decay: 0.0\n",
      "2020-12-17 03:08:31.897 | DEBUG    | theta.modeling.utils:debug:26 - ignore_categories: None\n",
      "2020-12-17 03:08:31.900 | DEBUG    | theta.modeling.utils:debug:26 - ner_labels: []\n",
      "2020-12-17 03:08:31.901 | DEBUG    | theta.modeling.utils:debug:26 - ner_type: span\n",
      "2020-12-17 03:08:31.901 | DEBUG    | theta.modeling.utils:debug:26 - no_crf_loss: False\n",
      "2020-12-17 03:08:31.902 | DEBUG    | theta.modeling.utils:debug:26 - soft_label: False\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Params --------------------\n",
    "from theta.modeling import NerAppParams\n",
    "experiment_params = NerAppParams()\n",
    "\n",
    "# 在区域修改参数\n",
    "# ----------------------------------------\n",
    "# 8438bcf6\n",
    "# a094ef08 ENABLE_KD=True\n",
    "LR = 2e-5\n",
    "ADAM_EPS = 1e-8\n",
    "N_AUGS = 2\n",
    "N_EPOCHS = 6\n",
    "MAX_SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 32\n",
    "SEG_LEN = MAX_SEQ_LENGTH - 2\n",
    "SEG_BACKOFF = 0\n",
    "ENABLE_KD = False\n",
    "MODEL_PATH = \"/opt/share/pretrained/pytorch/bert-base-chinese\"\n",
    "#MODEL_PATH = \"/opt/share/pretrained/pytorch/roberta-wwm-large-ext-chinese\"\n",
    "CONFIDENCE = 0.0\n",
    "LOSS_TYPE = \"CrossEntropyLoss\"\n",
    "FOCALLOSS_GAMMA = 1.5\n",
    "ALLOW_OVERLAP = False\n",
    "NER_TYPE = \"span\"\n",
    "SOFT_LABEL = False\n",
    "ENABLE_NESTED_ENTITIES = False\n",
    "FP16 = True\n",
    "CC = None\n",
    "\n",
    "SEED = 42\n",
    "FOLD = 0\n",
    "\n",
    "# ----------------------------------------\n",
    "# 9488ff16\n",
    "# LR = 1e-4\n",
    "# ADAM_EPS = 1e-6\n",
    "# N_AUGS = 0\n",
    "# N_EPOCHS = 6\n",
    "# MAX_SEQ_LENGTH = 64\n",
    "# BATCH_SIZE = 32\n",
    "# SEG_LEN = MAX_SEQ_LENGTH - 2\n",
    "# SEG_BACKOFF = 0\n",
    "# ENABLE_KD = False\n",
    "# MODEL_PATH = \"/opt/share/pretrained/pytorch/bert-base-chinese\"\n",
    "# CONFIDENCE = 0.35\n",
    "# LOSS_TYPE = \"CrossEntropyLoss\"\n",
    "# FOCALLOSS_GAMMA = 2.0\n",
    "# ALLOW_OVERLAP = False\n",
    "# NER_TYPE = \"pn\"\n",
    "# SOFT_LABEL = False\n",
    "# ENABLE_NESTED_ENTITIES = False\n",
    "# FP16 = True\n",
    "# CC = None\n",
    "\n",
    "# SEED = 8864\n",
    "# FOLD = 0\n",
    "\n",
    "# ----------------------------------------\n",
    "# 以下无需修改\n",
    "\n",
    "conf_common_params = {\n",
    "    'dataset_name': \"cluener\",\n",
    "    'experiment_name': \"CLUE\",\n",
    "    'learning_rate': LR,\n",
    "    'adam_epsilon': ADAM_EPS,\n",
    "    'fold': FOLD,\n",
    "    'num_augments': N_AUGS,\n",
    "    'enable_kd': ENABLE_KD,\n",
    "    'num_train_epochs': N_EPOCHS,\n",
    "    'train_max_seq_length': MAX_SEQ_LENGTH,\n",
    "    'eval_max_seq_length': MAX_SEQ_LENGTH,\n",
    "    'per_gpu_train_batch_size': BATCH_SIZE,\n",
    "    'per_gpu_eval_batch_size': BATCH_SIZE,\n",
    "    'per_gpu_predict_batch_size': BATCH_SIZE,\n",
    "    'seg_len': SEG_LEN,\n",
    "    'seg_backoff': SEG_BACKOFF,\n",
    "    'model_path': MODEL_PATH,\n",
    "    'confidence': CONFIDENCE,\n",
    "    'loss_type': LOSS_TYPE,\n",
    "    'focalloss_gamma': FOCALLOSS_GAMMA,\n",
    "    'allow_overlap': ALLOW_OVERLAP,\n",
    "    'enable_nested_entities': ENABLE_NESTED_ENTITIES,\n",
    "    'fp16': FP16,\n",
    "    'cc': CC,\n",
    "    'seed': SEED\n",
    "}\n",
    "conf_ner_params = {'ner_type': NER_TYPE, 'soft_label': SOFT_LABEL}\n",
    "\n",
    "for k, v in conf_common_params.items():\n",
    "    setattr(experiment_params.common_params, k, v)\n",
    "for k, v in conf_ner_params.items():\n",
    "    setattr(experiment_params.ner_params, k, v)\n",
    "experiment_params.debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新版本实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple msgpack msgpack_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'base'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1ebbcd6cbe80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityDataFlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaggers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPointerSequenceTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/nlp/dataflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#File:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/nlp/dataflow/dataflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m        \u001b[0;32mnot\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__SKIP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0m_global_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/nlp/dataflow/dataflow/__init__.py\u001b[0m in \u001b[0;36m_global_import\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'__all__'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'base'"
     ]
    }
   ],
   "source": [
    "from theta.nlp.dataflow import EntityDataFlow\n",
    "from theta.nlp.taggers import PointerSequenceTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启动实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting msgpack\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8e/e8/a4b64266a1fb99190a3a03c999395108ef2fde5d912343b2a6cf435d59af/msgpack-1.0.1-cp37-cp37m-manylinux2010_x86_64.whl (273 kB)\n",
      "\u001b[K     |████████████████████████████████| 273 kB 118 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting msgpack_numpy\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/05/05b8d7c69c6abb36a34325cc3150089bdafc359f0a81fb998d93c5d5c737/msgpack_numpy-0.4.7.1-py2.py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from msgpack_numpy) (1.18.5)\n",
      "Installing collected packages: msgpack, msgpack-numpy\n",
      "Successfully installed msgpack-1.0.1 msgpack-numpy-0.4.7.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-17 03:13:42.218 | WARNING  | theta.modeling.common_args:ensure_latest_dir:480 - local_id: 24cc30fc\n",
      "2020-12-17 03:13:42.224 | WARNING  | theta.modeling.common_args:get_main_args:503 - dataset_name: cluener\n",
      "2020-12-17 03:13:42.225 | WARNING  | theta.modeling.common_args:get_main_args:504 - experiment_name: CLUE\n",
      "2020-12-17 03:13:42.226 | WARNING  | theta.modeling.common_args:get_main_args:505 - local_id: 24cc30fc\n",
      "2020-12-17 03:13:42.227 | WARNING  | theta.modeling.common_args:get_main_args:506 - local_dir: ./outputs/saved_models/24cc30fc\n",
      "2020-12-17 03:13:42.229 | WARNING  | theta.modeling.common_args:get_main_args:507 - latest_dir: ./outputs/latest\n",
      "2020-12-17 03:13:42.230 | WARNING  | theta.modeling.common_args:get_main_args:508 - experiments_dir: ./experiments\n",
      "2020-12-17 03:13:42.230 | WARNING  | theta.modeling.common_args:get_main_args:509 - saved_models_path: ./outputs/saved_models\n",
      "2020-12-17 03:13:42.232 | INFO     | theta.modeling.app:__init__:278 - args: Namespace(adam_epsilon=1e-08, allow_overlap=False, app_type='ner', artifact_path=None, aug_train_only=False, best_index='f1', best_model_path='./outputs/latest/best', brat_data_dir=None, cache_dir=None, cache_features=False, cc=None, confidence=0.0, data_dir=None, dataset_file=None, dataset_name='cluener', diceloss_weight=None, do_eda=False, do_eval=False, do_experiment=True, do_lower_case=False, do_new=False, do_predict=False, do_submit=False, do_train=False, emotion_words_file=None, enable_kd=False, enable_nested_entities=False, enable_sda=False, eval_all_checkpoints=False, eval_file=None, eval_max_seq_length=64, evaluate_during_training=False, experiment_id=None, experiment_name='CLUE', experiments_dir='./experiments', focalloss_alpha=None, focalloss_gamma=1.5, fold=0, fp16=True, fp16_opt_level='O1', generate_submission=False, gradient_accumulation_steps=1, ignore_categories=None, is_english=False, kd_coeff=1.0, kd_decay=0.995, latest_dir='./outputs/latest', learning_rate=2e-05, local_dir='./outputs/saved_models/24cc30fc', local_id='24cc30fc', local_id_file='./outputs/latest/local_id', local_rank=-1, logging_steps=50, loss_type='CrossEntropyLoss', max_grad_norm=1.0, max_pages=100, max_span_len=32, max_steps=-1, max_train_examples=0, model_id='24cc30fc', model_path='/opt/share/pretrained/pytorch/bert-base-chinese', model_type='bert', ner_labels=[], ner_type='span', no_crf_loss=False, no_cuda=False, no_eval_on_each_epoch=False, num_augments=2, num_labels=2, num_train_epochs=6, output_dir='./outputs', overwrite=False, overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=32, per_gpu_predict_batch_size=32, per_gpu_train_batch_size=32, preapre_data=False, predict_all_checkpoints=False, predict_max_seq_length=256, random_type=None, resume_train=False, reviews_file='./outputs/latest/cluener_reviews_24cc30fc.json', run_name=None, save_checkpoints=False, save_steps=50, saved_models_path='./outputs/saved_models', sda_coeff=1.0, sda_decay=0.995, sda_empty_first=False, sda_stategy='recent_models', sda_teachers=3, seed=42, seg_backoff=0, seg_len=62, server_ip='', server_port='', soft_label=False, start_page=0, submission_file=None, submissions_dir='./submissions', task_name='launcher', test_file=None, to_reviews_poplar=False, to_train_poplar=False, tracking_uri=None, train_file=None, train_max_seq_length=64, train_rate=0.9, train_sample_rate=1.0, warmup_rate=0.1, warmup_steps=0, weight_decay=0.0)\n",
      "2020-12-17 03:13:42.234 | INFO     | theta.modeling.ner_span.trainer:init_labels:493 - args.label2id: {'[unused1]': 0, 'address': 1, 'book': 2, 'company': 3, 'game': 4, 'government': 5, 'movie': 6, 'name': 7, 'organization': 8, 'position': 9, 'scene': 10}\n",
      "2020-12-17 03:13:42.235 | INFO     | theta.modeling.ner_span.trainer:init_labels:494 - args.id2label: {0: '[unused1]', 1: 'address', 2: 'book', 3: 'company', 4: 'game', 5: 'government', 6: 'movie', 7: 'name', 8: 'organization', 9: 'position', 10: 'scene'}\n",
      "2020-12-17 03:13:42.236 | INFO     | theta.modeling.ner_span.trainer:init_labels:495 - args.num_labels: 11\n",
      "2020-12-17 03:13:42.255 | WARNING  | theta.modeling.ner_utils:load_train_val_examples:883 - shuffle: True, train_rate: 0.90, num_augments: 2\n",
      "Train data: 100%|██████████| 10748/10748 [00:00<00:00, 45809.81it/s]\n",
      "Eval data: 100%|██████████| 1343/1343 [00:00<00:00, 47297.16it/s]\n",
      "2020-12-17 03:13:42.537 | WARNING  | theta.modeling.ner_utils:load_train_val_examples:909 - len(lines): 12091\n",
      "100%|██████████| 12091/12091 [00:00<00:00, 72967.45it/s]\n",
      "2020-12-17 03:13:42.706 | WARNING  | theta.modeling.ner_utils:data_seg_generator:413 - num_overlap: 0\n",
      "Augement 2X: 0it [00:00, ?it/s]\n",
      "2020-12-17 03:13:42.710 | INFO     | theta.modeling.ner_utils:load_ner_labeled_examples:503 - Loaded 12091 examples.\n",
      "2020-12-17 03:13:42.711 | WARNING  | theta.modeling.ner_utils:load_train_val_examples:923 - len(train_base_examples): 12091\n",
      "2020-12-17 03:13:42.726 | INFO     | theta.utils.utils:split_train_eval_examples:406 - num_train_examples: 10881, num_eval_examples: 1210\n",
      "2020-12-17 03:13:42.727 | WARNING  | theta.utils.utils:split_train_eval_examples:413 - Eval examples: [10881:12091]\n",
      "2020-12-17 03:13:42.728 | INFO     | theta.modeling.ner_utils:load_train_val_examples:932 - Loaded 10881 train examples, 1210 val examples.\n",
      "2020-12-17 03:13:42.741 | INFO     | theta.modeling.trainer:train:275 - Start train: 10881 train examples, 1210 eval examples.\n",
      "2020-12-17 03:13:42.742 | INFO     | theta.modeling.utils:save_args:245 - Save args in ./outputs/latest/training_args.json\n",
      "2020-12-17 03:13:42.744 | WARNING  | theta.modeling.trainer:save_params_file:250 - cp cluener_params.py ./outputs/latest/cluener_params.py\n",
      "Build enttype_labels: 100%|██████████| 12091/12091 [00:00<00:00, 249739.39it/s]\n",
      "Aug examples: 100%|██████████| 10881/10881 [00:00<00:00, 26177.38it/s]\n",
      "Aug examples: 100%|██████████| 21762/21762 [00:00<00:00, 70011.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4a9878cd672a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'--do_experiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-31aacd395402>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtest_data_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mgenerate_submission\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_submission\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             eval_data_generator=None)\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;31m#eval_data_generator=eval_data_generator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0abeafc1a2ea>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_data_generator, test_data_generator, generate_submission, eval_data_generator)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             super(MyApp, self).run(train_data_generator, test_data_generator,\n\u001b[0;32m---> 29\u001b[0;31m                                    generate_submission, eval_data_generator)\n\u001b[0m",
      "\u001b[0;32m/notebooks/theta/theta/modeling/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_data_generator, test_data_generator, generate_submission, eval_data_generator)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0;31m# ----- Train -----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0;31m# ----- Predict -----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/modeling/app.py\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    417\u001b[0m                         \u001b[0mnum_augments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                         aug_train_only=args.aug_train_only)\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mdo_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/modeling/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, args, train_examples, eval_examples)\u001b[0m\n\u001b[1;32m    298\u001b[0m                                           augs=args.num_augments)\n\u001b[1;32m    299\u001b[0m         all_features = self.encode_examples(train_examples,\n\u001b[0;32m--> 300\u001b[0;31m                                             args.train_max_seq_length)\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generate dataloader...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/modeling/ner_span/trainer.py\u001b[0m in \u001b[0;36mencode_examples\u001b[0;34m(self, examples, max_seq_length, enttype_labels, epoch)\u001b[0m\n\u001b[1;32m    529\u001b[0m                                \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                                \u001b[0menttype_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menttype_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                                epoch=epoch)\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_to_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/modeling/ner_span/dataset.py\u001b[0m in \u001b[0;36mencode_examples\u001b[0;34m(examples, label2id, tokenizer, max_seq_length, enttype_labels, epoch)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     all_tokens, all_token2char, all_char2token, all_input_ids, all_attention_mask, all_token_type_ids, all_input_lens, all_token_offsets = common_batch_encode(\n\u001b[0;32m--> 402\u001b[0;31m         texts, label2id, tokenizer, max_seq_length)\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_subjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/modeling/trainer.py\u001b[0m in \u001b[0;36mcommon_batch_encode\u001b[0;34m(texts, label2id, tokenizer, max_seq_length)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcommon_batch_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mall_encodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_encodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mall_token_offsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_encodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'offsets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/modeling/token_utils.py\u001b[0m in \u001b[0;36mbatch_encode\u001b[0;34m(self, texts, add_special_tokens)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mbatch_encodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mencodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mbatch_encodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/theta/theta/modeling/token_utils.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, max_seq_length, add_special_tokens)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         text_tokens = self._tokenizer.encode(\n\u001b[0;32m---> 55\u001b[0;31m             text, add_special_tokens=add_special_tokens)\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tokenizers/implementations/base_tokenizer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sequence, pair, is_pretokenized, add_special_tokens)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encode: `sequence` can't be `None`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     def encode_batch(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = [sys.argv[0]] + ['--do_experiment']\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theta.modeling import ner_evaluate\n",
    "dev_file=None\n",
    "reviews_file = \"outputs/latest/cluener_reviews_a094ef08.json\"\n",
    "macro_acc, macro_recall, macro_f1, micro_acc, micro_recall, micro_f1 = ner_evaluate(\n",
    "    dev_file, reviews_file, eval_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
